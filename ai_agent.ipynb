{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.1 Rag Agent with LlamaIndex\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ytang07/ai_agents_cookbooks/blob/main/llamaindex/llama31_8b_rag_agent.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This notebook will walk you through building a LlamaIndex ReactAgent using Llama 3.1 70b. We will be using [OctoAI](https://octo.ai) as our embeddings and llm provider.\n",
    "\n",
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -qU llama-index llama-index-llms-openai llama-index-readers-file octoai llama-index-llms-octoai llama-index-embeddings-octoai llama-index-embeddings-openai llama-index-llms-openai-like\n",
    "\n",
    "# ! pip freeze | grep llama-index-core\n",
    "# ! pip freeze | grep embeddings-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup API Keys\n",
    "To run the rest of the notebook you will need access to an OctoAI API key. You can sign up for an account [here](https://octoai.cloud/). If you need further guidance you can check OctoAI's [documentation page](https://octo.ai/docs/getting-started/how-to-create-octoai-access-token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from getpass import getpass\n",
    "environ[\"OCTOAI_API_KEY\"] = \"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6IjNkMjMzOTQ5In0.eyJzdWIiOiJjOTMwYzBkZC0zYzkyLTQyZDEtYTc1My1lN2ViNjgzOGU1MzIiLCJ0eXBlIjoidXNlckFjY2Vzc1Rva2VuIiwidGVuYW50SWQiOiJmMmRiOTI5Mi05NjM0LTQ1ZmEtYWUyOS05ODAyYTdkZWNkYzQiLCJ1c2VySWQiOiI3ZDk3ZTIzOS0xNzQzLTQ2MWUtYjRiOS05N2Q1YTAyNTYxNTEiLCJhcHBsaWNhdGlvbklkIjoiYTkyNmZlYmQtMjFlYS00ODdiLTg1ZjUtMzQ5NDA5N2VjODMzIiwicm9sZXMiOlsiRkVUQ0gtUk9MRVMtQlktQVBJIl0sInBlcm1pc3Npb25zIjpbIkZFVENILVBFUk1JU1NJT05TLUJZLUFQSSJdLCJhdWQiOiIzZDIzMzk0OS1hMmZiLTRhYjAtYjdlYy00NmY2MjU1YzUxMGUiLCJpc3MiOiJodHRwczovL2lkZW50aXR5Lm9jdG8uYWkiLCJpYXQiOjE3MjQ1MjE3NDZ9.MfKO6we42NX5vLGuVZwwCo46x4nT8BUX42gVGK3YcWZ3nLGye98hIgVC_p0cYelVRvg6yfxTXo--XL-0NJ3Nc3A6W9IvpiKZLhW9oBq2QzSGZxn3yK-0JqNANPqp6BRiV6V-dcu_XgL2fN3rVhkaDHJU-MIXRsnVxfESc1Ks2G_jpTDMmYRDSx3vNkoQzHTOJ5FsdVcdD33E6LzbHC_Q7hhlqUsaLgR-WwP4p6HchaQlo_2aTvCiLmG1xy8vZlcWJ-DmQoXU-BYuHubTyYhZ8gpeC3_nuqMgZW_pVPd-Y38EO9eewqlRfYLo8XZdLlqCYcCkbfHspfBjgKrn01tQIw\"\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OCTOAI_API_KEY = environ[\"OCTOAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and setup LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.embeddings.octoai import OctoAIEmbedding\n",
    "from llama_index.core import Settings as LlamaGlobalSettings\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "\n",
    "# Set the default model to use for embeddings\n",
    "LlamaGlobalSettings.embed_model = OctoAIEmbedding()\n",
    "\n",
    "# Create an llm object to use for the QueryEngine and the ReActAgent\n",
    "llm = OpenAILike(\n",
    "    model=\"meta-llama-3.1-70b-instruct\",\n",
    "    api_base=\"https://text.octoai.run/v1\",\n",
    "    api_key=environ[\"OCTOAI_API_KEY\"],\n",
    "    context_window=40000,\n",
    "    is_function_calling_model=True,\n",
    "    is_chat_model=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        persist_dir=\"./storage2/msim\"\n",
    "    )\n",
    "    msim_index = load_index_from_storage(storage_context)\n",
    "\n",
    "    index_loaded = True\n",
    "except:\n",
    "    index_loaded = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the point we create our vector indexes, by calculating the embedding vectors for each of the chunks. You only need to run this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0244f5e185d54e7bbde9638f61438636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2544f0c8f649168ea8ded56f869d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "if not index_loaded:\n",
    "    # load data\n",
    "    msim_docs = SimpleDirectoryReader(\n",
    "        input_files=[\"pdfs/\"+ x for x in os.listdir(\"pdfs/\")], filename_as_id=True).load_data()\n",
    "    print(\"hello\")\n",
    "    # build index\n",
    "    msim_index = VectorStoreIndex.from_documents(msim_docs, show_progress=True)\n",
    "\n",
    "    # persist index\n",
    "    msim_index.storage_context.persist(persist_dir=\"./storage2/msim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import CitationQueryEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the query engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "msim_engine = CitationQueryEngine.from_args(\n",
    "    msim_index,\n",
    "    similarity_top_k=3,\n",
    "    citation_chunk_size=1024,llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the query engines as tools that will be used by the agent.\n",
    "\n",
    "As there is a query engine per document we need to also define one tool for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=msim_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"msim\",\n",
    "            description=(\n",
    "                \"Provides information about MSIM program \"\n",
    "                \"Use a detailed plain text question as input to the tool.\"\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Agent\n",
    "Now we have all the elements to create a LlamaIndex ReactAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    llm=llm,\n",
    "    max_turns=10,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can interact with the agent and ask a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step dde75dbb-fc67-477e-9c58-8e11eda7b143. Step input: What are the different tracks of MSIM? Cite the file of the source\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: msim\n",
      "Action Input: {'input': 'MSIM tracks'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: The MSIM program is offered on three tracks: Early-Career, Early-Career Accelerated, and Mid-Career [3].\n",
      "\u001b[0m> Running step c549df63-c192-4eb9-bb5b-b151293a92a2. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The different tracks of MSIM are Early-Career, Early-Career Accelerated, and Mid-Career.\n",
      "\u001b[0mThe different tracks of MSIM are Early-Career, Early-Career Accelerated, and Mid-Career.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What are the different tracks of MSIM? Cite the file of the source\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OctoAI API key:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded successfully.\n",
      "Query engine and agent set up successfully.\n"
     ]
    }
   ],
   "source": [
    "from os import environ, listdir, path\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.embeddings.octoai import OctoAIEmbedding\n",
    "from llama_index.core import Settings as LlamaGlobalSettings\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.core.query_engine import CitationQueryEngine\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set API Key for OctoAI\n",
    "environ[\"OCTOAI_API_KEY\"] = \"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6IjNkMjMzOTQ5In0.eyJzdWIiOiJjOTMwYzBkZC0zYzkyLTQyZDEtYTc1My1lN2ViNjgzOGU1MzIiLCJ0eXBlIjoidXNlckFjY2Vzc1Rva2VuIiwidGVuYW50SWQiOiJmMmRiOTI5Mi05NjM0LTQ1ZmEtYWUyOS05ODAyYTdkZWNkYzQiLCJ1c2VySWQiOiI3ZDk3ZTIzOS0xNzQzLTQ2MWUtYjRiOS05N2Q1YTAyNTYxNTEiLCJhcHBsaWNhdGlvbklkIjoiYTkyNmZlYmQtMjFlYS00ODdiLTg1ZjUtMzQ5NDA5N2VjODMzIiwicm9sZXMiOlsiRkVUQ0gtUk9MRVMtQlktQVBJIl0sInBlcm1pc3Npb25zIjpbIkZFVENILVBFUk1JU1NJT05TLUJZLUFQSSJdLCJhdWQiOiIzZDIzMzk0OS1hMmZiLTRhYjAtYjdlYy00NmY2MjU1YzUxMGUiLCJpc3MiOiJodHRwczovL2lkZW50aXR5Lm9jdG8uYWkiLCJpYXQiOjE3MjQ1MjE3NDZ9.MfKO6we42NX5vLGuVZwwCo46x4nT8BUX42gVGK3YcWZ3nLGye98hIgVC_p0cYelVRvg6yfxTXo--XL-0NJ3Nc3A6W9IvpiKZLhW9oBq2QzSGZxn3yK-0JqNANPqp6BRiV6V-dcu_XgL2fN3rVhkaDHJU-MIXRsnVxfESc1Ks2G_jpTDMmYRDSx3vNkoQzHTOJ5FsdVcdD33E6LzbHC_Q7hhlqUsaLgR-WwP4p6HchaQlo_2aTvCiLmG1xy8vZlcWJ-DmQoXU-BYuHubTyYhZ8gpeC3_nuqMgZW_pVPd-Y38EO9eewqlRfYLo8XZdLlqCYcCkbfHspfBjgKrn01tQIw\"\n",
    "\n",
    "# Set the default model to use for embeddings\n",
    "LlamaGlobalSettings.embed_model = OctoAIEmbedding()\n",
    "\n",
    "# Create an LLM object to use for the QueryEngine and the ReActAgent\n",
    "llm = OpenAILike(\n",
    "    model=\"meta-llama-3.1-70b-instruct\",\n",
    "    api_base=\"https://text.octoai.run/v1\",\n",
    "    api_key=environ[\"OCTOAI_API_KEY\"],\n",
    "    context_window=40000,\n",
    "    is_function_calling_model=True,\n",
    "    is_chat_model=True,\n",
    ")\n",
    "\n",
    "# Set up storage and loading of the index\n",
    "try:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage/msim\")\n",
    "    msim_index = load_index_from_storage(storage_context)\n",
    "    print(\"Index loaded successfully.\")\n",
    "    index_loaded = True\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load index: {e}\")\n",
    "    index_loaded = False\n",
    "\n",
    "# Load documents from the 'pdfs' directory if the index isn't loaded\n",
    "if not index_loaded:\n",
    "    try:\n",
    "        pdf_dir = \"pdfs/\"\n",
    "        \n",
    "        # Check if the directory exists and contains files\n",
    "        if not path.exists(pdf_dir):\n",
    "            raise FileNotFoundError(f\"Directory does not exist: {pdf_dir}\")\n",
    "        \n",
    "        pdf_files = [f for f in listdir(pdf_dir) if path.isfile(path.join(pdf_dir, f))]\n",
    "        \n",
    "        if not pdf_files:\n",
    "            raise FileNotFoundError(f\"No PDF files found in directory: {pdf_dir}\")\n",
    "        \n",
    "        # Load documents using SimpleDirectoryReader\n",
    "        msim_docs = SimpleDirectoryReader(input_files=[path.join(pdf_dir, x) for x in pdf_files], filename_as_id=True).load_data()\n",
    "        \n",
    "        # Debugging print: Check if documents are loaded properly\n",
    "        print(f\"Loaded documents: {[doc.metadata['filename'] for doc in msim_docs]}\")\n",
    "\n",
    "        # Build the index from the loaded documents\n",
    "        msim_index = VectorStoreIndex.from_documents(msim_docs, show_progress=True)\n",
    "        print(\"Index built successfully.\")\n",
    "        \n",
    "        # Persist the index for future use\n",
    "        msim_index.storage_context.persist(persist_dir=\"./storage/msim\")\n",
    "        print(\"Index persisted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF files or building the index: {e}\")\n",
    "\n",
    "# Create the query engine with proper citation handling\n",
    "try:\n",
    "    query_engine = CitationQueryEngine.from_args(\n",
    "        msim_index,\n",
    "        similarity_top_k=3,\n",
    "        citation_chunk_size=1024,\n",
    "        llm=llm,\n",
    "        citation_formatter=lambda doc_ids: [doc.metadata['filename'] for doc in msim_docs if doc.metadata['doc_id'] in doc_ids]\n",
    "    )\n",
    "    msim_engine = CitationQueryEngine.from_args(\n",
    "        msim_index,\n",
    "        similarity_top_k=3,\n",
    "        citation_chunk_size=1024,\n",
    "        llm=llm,\n",
    "        citation_formatter=lambda doc_ids: [doc.metadata['filename'] for doc in msim_docs if doc.metadata['doc_id'] in doc_ids]\n",
    "    )\n",
    "\n",
    "    query_engine_tools = [\n",
    "        QueryEngineTool(\n",
    "            query_engine=msim_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"msim\",\n",
    "                description=(\n",
    "                    \"Provides information about MSIM program \"\n",
    "                    \"Use a detailed plain text question as input to the tool.\"\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Set up the agent with the tools\n",
    "    new_agent = ReActAgent.from_tools(\n",
    "        query_engine_tools,\n",
    "        llm=llm,\n",
    "        max_turns=10,\n",
    "        verbose=True\n",
    "    )\n",
    "    print(\"Query engine and agent set up successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up query engine or agent: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 15ae44c1-b5dc-49fb-8b47-f61a15cbbaca. Step input: What are the different tracks of MSIM? Cite the file of the source\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: msim\n",
      "Action Input: {'input': 'MSIM tracks'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: The MSIM program is offered on three tracks: Early-Career, Early-Career Accelerated, and Mid-Career [3].\n",
      "\u001b[0m> Running step c1329057-dc1f-42a9-b252-94d67ffdd340. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The different tracks of MSIM are Early-Career, Early-Career Accelerated, and Mid-Career.\n",
      "\u001b[0mThe different tracks of MSIM are Early-Career, Early-Career Accelerated, and Mid-Career.\n"
     ]
    }
   ],
   "source": [
    "response = new_agent.chat(\"What are the different tracks of MSIM? Cite the file of the source\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded successfully.\n",
      "Query engine and agent set up successfully.\n"
     ]
    }
   ],
   "source": [
    "from os import environ, listdir, path\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.embeddings.octoai import OctoAIEmbedding\n",
    "from llama_index.core import Settings as LlamaGlobalSettings\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.core.query_engine import CitationQueryEngine\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set API Key for OctoAI\n",
    "environ[\"OCTOAI_API_KEY\"] = \"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6IjNkMjMzOTQ5In0.eyJzdWIiOiJjOTMwYzBkZC0zYzkyLTQyZDEtYTc1My1lN2ViNjgzOGU1MzIiLCJ0eXBlIjoidXNlckFjY2Vzc1Rva2VuIiwidGVuYW50SWQiOiJmMmRiOTI5Mi05NjM0LTQ1ZmEtYWUyOS05ODAyYTdkZWNkYzQiLCJ1c2VySWQiOiI3ZDk3ZTIzOS0xNzQzLTQ2MWUtYjRiOS05N2Q1YTAyNTYxNTEiLCJhcHBsaWNhdGlvbklkIjoiYTkyNmZlYmQtMjFlYS00ODdiLTg1ZjUtMzQ5NDA5N2VjODMzIiwicm9sZXMiOlsiRkVUQ0gtUk9MRVMtQlktQVBJIl0sInBlcm1pc3Npb25zIjpbIkZFVENILVBFUk1JU1NJT05TLUJZLUFQSSJdLCJhdWQiOiIzZDIzMzk0OS1hMmZiLTRhYjAtYjdlYy00NmY2MjU1YzUxMGUiLCJpc3MiOiJodHRwczovL2lkZW50aXR5Lm9jdG8uYWkiLCJpYXQiOjE3MjQ1MjE3NDZ9.MfKO6we42NX5vLGuVZwwCo46x4nT8BUX42gVGK3YcWZ3nLGye98hIgVC_p0cYelVRvg6yfxTXo--XL-0NJ3Nc3A6W9IvpiKZLhW9oBq2QzSGZxn3yK-0JqNANPqp6BRiV6V-dcu_XgL2fN3rVhkaDHJU-MIXRsnVxfESc1Ks2G_jpTDMmYRDSx3vNkoQzHTOJ5FsdVcdD33E6LzbHC_Q7hhlqUsaLgR-WwP4p6HchaQlo_2aTvCiLmG1xy8vZlcWJ-DmQoXU-BYuHubTyYhZ8gpeC3_nuqMgZW_pVPd-Y38EO9eewqlRfYLo8XZdLlqCYcCkbfHspfBjgKrn01tQIw\"\n",
    "\n",
    "# Set the default model to use for embeddings\n",
    "LlamaGlobalSettings.embed_model = OctoAIEmbedding()\n",
    "\n",
    "# Create an LLM object to use for the QueryEngine and the ReActAgent\n",
    "llm = OpenAILike(\n",
    "    model=\"meta-llama-3.1-70b-instruct\",\n",
    "    api_base=\"https://text.octoai.run/v1\",\n",
    "    api_key=environ[\"OCTOAI_API_KEY\"],\n",
    "    context_window=40000,\n",
    "    is_function_calling_model=True,\n",
    "    is_chat_model=True,\n",
    ")\n",
    "\n",
    "# Set up storage and loading of the index\n",
    "try:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage/msim\")\n",
    "    msim_index = load_index_from_storage(storage_context)\n",
    "    print(\"Index loaded successfully.\")\n",
    "    index_loaded = True\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load index: {e}\")\n",
    "    index_loaded = False\n",
    "\n",
    "# Load documents from the 'pdfs' directory if the index isn't loaded\n",
    "if not index_loaded:\n",
    "    try:\n",
    "        pdf_dir = \"pdfs/\"\n",
    "        \n",
    "        # Check if the directory exists and contains files\n",
    "        if not path.exists(pdf_dir):\n",
    "            raise FileNotFoundError(f\"Directory does not exist: {pdf_dir}\")\n",
    "        \n",
    "        pdf_files = [f for f in listdir(pdf_dir) if path.isfile(path.join(pdf_dir, f))]\n",
    "        \n",
    "        if not pdf_files:\n",
    "            raise FileNotFoundError(f\"No PDF files found in directory: {pdf_dir}\")\n",
    "        \n",
    "        # Load documents using SimpleDirectoryReader\n",
    "        msim_docs = SimpleDirectoryReader(input_files=[path.join(pdf_dir, x) for x in pdf_files], filename_as_id=True).load_data()\n",
    "        \n",
    "        # Debugging print: Check if documents are loaded properly\n",
    "        print(f\"Loaded documents: {[doc.metadata['filename'] for doc in msim_docs]}\")\n",
    "\n",
    "        # Build the index from the loaded documents\n",
    "        msim_index = VectorStoreIndex.from_documents(msim_docs, show_progress=True)\n",
    "        print(\"Index built successfully.\")\n",
    "        \n",
    "        # Persist the index for future use\n",
    "        msim_index.storage_context.persist(persist_dir=\"./storage/msim\")\n",
    "        print(\"Index persisted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF files or building the index: {e}\")\n",
    "\n",
    "# Create the query engine with proper citation handling\n",
    "def format_citations(doc_ids):\n",
    "    citations = []\n",
    "    for doc_id in doc_ids:\n",
    "        # Match doc_id with the loaded documents\n",
    "        for doc in msim_docs:\n",
    "            if doc.metadata['doc_id'] == doc_id:\n",
    "                citations.append(doc.metadata['filename'])\n",
    "    return citations\n",
    "\n",
    "try:\n",
    "    query_engine = CitationQueryEngine.from_args(\n",
    "        msim_index,\n",
    "        similarity_top_k=3,\n",
    "        citation_chunk_size=1024,\n",
    "        llm=llm,\n",
    "        citation_formatter=format_citations  # Use custom citation formatter\n",
    "    )\n",
    "    msim_engine = CitationQueryEngine.from_args(\n",
    "        msim_index,\n",
    "        similarity_top_k=3,\n",
    "        citation_chunk_size=1024,\n",
    "        llm=llm,\n",
    "        citation_formatter=format_citations  # Use custom citation formatter\n",
    "    )\n",
    "\n",
    "    query_engine_tools = [\n",
    "        QueryEngineTool(\n",
    "            query_engine=msim_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"msim\",\n",
    "                description=(\n",
    "                    \"Provides information about MSIM program \"\n",
    "                    \"Use a detailed plain text question as input to the tool.\"\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Set up the agent with the tools\n",
    "    news_agent = ReActAgent.from_tools(\n",
    "        query_engine_tools,\n",
    "        llm=llm,\n",
    "        max_turns=10,\n",
    "        verbose=True\n",
    "    )\n",
    "    print(\"Query engine and agent set up successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up query engine or agent: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step bfda174d-6242-4779-880f-a0119a083274. Step input: What are the different tracks of MSIM? Cite the file of the source\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: msim\n",
      "Action Input: {'input': 'MSIM tracks'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: The MSIM program is offered on three tracks: Early-Career, Early-Career Accelerated, and Mid-Career [3].\n",
      "\u001b[0m> Running step 29a2c860-908e-46a7-a590-578c4392c1dc. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The different tracks of MSIM are Early-Career, Early-Career Accelerated, and Mid-Career.\n",
      "\u001b[0mThe different tracks of MSIM are Early-Career, Early-Career Accelerated, and Mid-Career.\n"
     ]
    }
   ],
   "source": [
    "response = news_agent.chat(\"What are the different tracks of MSIM? Cite the file of the source\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
